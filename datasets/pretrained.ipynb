{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#載入數據\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "#載入資料\n",
    "dataset = load_dataset(\"csv\",data_files=\"./ChnSentiCorp_htl_all.csv\",split=\"train\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#清理資料\n",
    "cleared_dataset = dataset.filter(lambda item: item['review'] is not None)\n",
    "cleared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#拆分資料集\n",
    "from datasets import Dataset\n",
    "datasets = cleared_dataset.train_test_split(train_size=0.9,test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0732995d84bf4ada9ca646c4294ca945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "def process_tokenizer(example:dict)->dict:\n",
    "    '''\n",
    "    只要先分詞,不要現在轉成tensor,轉成tensor,由DataCollator來作\n",
    "    '''\n",
    "    tokenized = tokenizer(example['review'],max_length=128,truncation=True)\n",
    "    tokenized['label'] = example['label']\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenizer_dataset = datasets.map(function=process_tokenizer,remove_columns=cleared_dataset.column_names)\n",
    "tokenizer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer,return_tensors='pt')\n",
    "collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4684, 2533,  ..., 3298, 7540,  102],\n",
       "        [ 101, 1184, 5637,  ..., 1217,  802,  102],\n",
       "        [ 101, 2769, 6221,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101,  127, 3299,  ..., 7279, 3300,  102],\n",
       "        [ 101, 4692,  749,  ..., 8024, 2523,  102],\n",
       "        [ 101, 7478, 2382,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "\n",
    "trainset, validset = tokenizer_dataset['train'], tokenizer_dataset['test']\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True,collate_fn=collator) #collate_fn,建立處理batch內的資料\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False,collate_fn=collator)\n",
    "\n",
    "next(enumerate(trainloader))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese')\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，train() 主要用於將模型設置為「訓練模式」，但 不會執行實際的訓練過程。一般來說，完整的訓練過程需要包含以下幾個步驟：\n",
    "1.\t設定模型為訓練模式 (model.train())\n",
    "2.\t定義損失函數與優化器\n",
    "3.\t讀取訓練資料\n",
    "4.\t前向傳播 (Forward Pass)\n",
    "5.\t計算損失\n",
    "6.\t反向傳播 (Backward Pass)\n",
    "7.\t更新權重\n",
    "\n",
    "\n",
    "### model.train() 的作用\n",
    "\n",
    "model.train() 用來將模型切換為「訓練模式」，影響 某些特定層的行為，例如：\n",
    "\t•\tDropout：啟用隨機失活 (Dropout)，防止過擬合\n",
    "\t•\tBatch Normalization：使用 mini-batch 內的統計數據 (均值、標準差) 來標準化輸入\n",
    "\n",
    "當進行評估時，需要呼叫 model.eval()，讓模型進入測試模式。\n",
    "\n",
    "2. PyTorch 訓練流程範例\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "```\n",
    "\n",
    "# 1. 建立簡單模型\n",
    "```\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout 層\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # 只有在 train() 模式下才會作用\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "# 2. 準備數據\n",
    "\n",
    "```\n",
    "x_train = torch.rand(100, 10)\n",
    "y_train = torch.rand(100, 1)\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "```\n",
    "\n",
    "# 3. 設定模型、損失函數、優化器\n",
    "```\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "# 4. 訓練模型\n",
    "```\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 設定為訓練模式\n",
    "\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        outputs = model(batch_x)  # 前向傳播\n",
    "        loss = criterion(outputs, batch_y)  # 計算損失\n",
    "        loss.backward()  # 反向傳播\n",
    "        optimizer.step()  # 更新權重\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"訓練完成\")\n",
    "```\n",
    "\n",
    "3. train() 和 eval() 的區別\n",
    "\n",
    "```\n",
    "模式\t使用的函數\tDropout\tBatch Normalization\t反向傳播\n",
    "訓練模式\tmodel.train()\t啟用\t使用 mini-batch 統計數據\t是\n",
    "測試模式\tmodel.eval()\t停用\t使用全局統計數據\t否\n",
    "\n",
    "在測試（評估）時，需確保模型進入 eval() 模式：\n",
    "\n",
    "model.eval()  # 設定為測試模式\n",
    "with torch.no_grad():  # 禁用梯度計算，節省記憶體與運算\n",
    "    test_output = model(test_input)\n",
    "```\n",
    "\n",
    "4. 重要注意事項\n",
    "- train() 不會執行訓練，它只是將模型設置為訓練模式。\n",
    "- 訓練時，請確保：\n",
    "- 調用 train()，確保 Dropout 和 BatchNorm 作用正確。\n",
    "- 調用 optimizer.zero_grad()，清空梯度，避免影響下一次計算。\n",
    "- 調用 loss.backward()，進行梯度反向傳播。\n",
    "- 調用 optimizer.step()，更新權重。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate()->float:\n",
    "  model.eval() #讓模型進入評估模式\n",
    "  validset_total = len(validset) #評估的筆數\n",
    "  correct = 0 #預測正確的筆數\n",
    "  for batch in validloader: #一次評估一個批次,目前每批次64筆\n",
    "    if torch.cuda.is_available():\n",
    "      batch = {k:v.cuda() for k, v in batch.items()}\n",
    "    output = model(**batch)\n",
    "    pred = torch.argmax(output.logits, dim=-1) #會有64個預測值\n",
    "    correct += (pred.long() == batch['labels'].long()).float().sum() #每一批次正確的筆數\n",
    "  acc = correct / validset_total #計算精準度\n",
    "  return acc\n",
    "\n",
    "def train(epoch=3, log_step=100):\n",
    "  global_step = 0\n",
    "  for ep in range(epoch):\n",
    "    model.train() #讓模型進入訓練模式\n",
    "    for batch in trainloader: #一次訓練一個批次,目前每批次32筆      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = {k:v.cuda() for k, v in batch.items()}\n",
    "      optimizer.zero_grad() #模型參數的梯度歸零\n",
    "      output = model(**batch)\n",
    "      output.loss.backward() #計算梯度\n",
    "      optimizer.step() #更新模型參數\n",
    "      if global_step % log_step == 0: #每100個批次,輸出一次損失梯度\n",
    "        print(f\"第{ep+1}躺,執行第{global_step}個批次,loss:{output.loss.item()}\")\n",
    "      global_step += 1 #每一批次就加1\n",
    "    \n",
    "    #每訓練一躺就評估一次精準度\n",
    "    acc = evaluate()\n",
    "    print(f\"第{ep+1}躺,精準度:{acc}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#儲存model,tokenizer,dataset於本地端\n",
    "model.save_pretrained('./save_model') #save_model是資料夾名稱\n",
    "tokenizer.save_pretrained('./save_model')\n",
    "datasets.save_to_disk('./save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上傳model,tokenize和dataset\n",
    "repo_id = \"roberthsu2003/save_model\"\n",
    "model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)\n",
    "datasets.push_to_hub(repo_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
