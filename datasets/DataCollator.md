## DataCollator
- DataCollator可以將Dataset資料分為批次訓練,而不是一次全部訓練

**DataCollatorWithPadding**
- 每一批次自動產生對應的padding
- 所以每一批次會依據內容,產生不同的批次

```python
from transformers import DataCollatorWithPadding
from datasets import load_dataset


dataset = load_dataset("csv",data_files="./ChnSentiCorp_htl_all.csv",split="train")
dataset

#==output==
dataset = load_dataset("csv",data_files="./ChnSentiCorp_htl_all.csv",split="train")
dataset
```

**清理資料**

```python
#清理資料
cleared_dataset = dataset.filter(lambda item: item['review'] is not None)
cleared_dataset

#==output==
Dataset({
    features: ['label', 'review'],
    num_rows: 7765
})
```

**分詞每筆資料**

```python
from transformers import AutoTokenizer
import torch

from transformers import AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')
def process_tokenizer(example:dict)->dict:
    '''
    只要先分詞,不要現在轉成tensor,轉成tensor,由DataCollator來作
    '''
    tokenized = tokenizer(example['review'],max_length=128,truncation=True)
    tokenized['label'] = example['label']
    return tokenized


tokenizer_dataset = cleared_dataset.map(function=process_tokenizer,remove_columns=cleared_dataset.column_names)
tokenizer_dataset[:2]
      
```

```python
collator = DataCollatorWithPadding(tokenizer=tokenizer)
collator

#==output==
DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')
```


```python
from torch.utils.data import DataLoader
d1 = DataLoader(tokenizer_dataset, batch_size=4, collate_fn=collator, shuffle=True)

next(enumerate(d1))[1] #可以發現已經轉成tensor

#==output==
{'input_ids': tensor([[  101,  2769,   947,  3176,  8126,  3189,   678,  1286,  2537,  6832,
          3172,  2225,  1962,  5844,  1854,  6983,  2421,   889,  1168,  2356,
          1281,  1057,   857,  6857,  2157,  7613,  2421,  8024,  7613,  2421,
          1762,   671,   943,   100,   100,  6174,  7481,  8024,  3760,   889,
          6882,  4638,  6917,  4696,   679,  6210,  2533,  5543,  2523,  3175,
           912,  4638,  2823,  1168,   511,  6794,  4415,  4638,  6862,  2428,
          2523,  2571,  8024,  2706,  2428,   738,  2523,  1962,  8024,  1920,
          5147,   126,  1146,  7132,  2218,  3018,  2137,   511,   889,  1168,
          2791,  7279,  2527,  6366,  2769,   947,  3300,  7953,   679,  6900,
          2746,  8024,  1728,  4158,  4696,  4638,  2523,  2207,  8024,  2215,
          1071,  3221,  1190,   857,  6882,  6832,  3172,  2225,  1962,  5844,
          1854,  6983,  2421,  8024,  2791,  7279,  7481,  4948,  1920,  5147,
          1372,  3300,  8108,  2398,  5101,  2340,  1381,   102],
        [  101,  2791,  7279,  3683,  6733,   746,  3912,  8024,  6127,  4495,
          7279,  1922,  2207,  8024,  6752,  6716,  6963,  6527,  1233,   511,
          1765,  4415,   855,  5390,   671,  1288,  8024,  1728,  1762,   100,
           100,  6174,  8024,   679,  2159,  3211,  2823,  1168,   511,  3302,
          1243,   671,  5663,   511,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  2791,  7279,  7478,  2382,  1920,  8024,  4801,  7768,  6257,
          3177,  6917,   679,  7097,  8024,   679,  6882,  3193,  7623,  2345,
           749,  7953,  8024,  2791,  1019,   738,  2253,  3176,  3683,  6733,
          6523,  4638,  6540,  7631,  1353,  7637,  8182,  2399,   126,  3299,
          8113,  3189,  8038,  2644,  1962,  8013,  2697,  6342,  2644,  4638,
          2456,  2692,  8024,  6983,  2421,  4638,  3193,  7623,  1139,  1501,
          4412,  3633,   976,  6310,  3146,  8024,  3309,  3307,  2644,  4638,
           678,  3613,  1045,  5631,  3298,  6888,  1168,  2644,  4638,  4021,
          2692,  8013,  6296,  3041,  4638,  3631,  6816,  2644,  4638,   678,
          3613,  1045,  5631,  8013,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  7521,  6242,  3862,  3250,  5709,  1754,  1920,  6983,  2421,
          2347,  5195,  3760,  3300,  2791,  7279,   749,  8024,  3176,  3221,
          2994,  1168,   749,  6857,  6174,  8024,  4692,  1168,  1912,  6134,
          3300,   763,  4542,  2663,  8024,  1728,  4158,  6983,  2421,   699,
           679,  1920,  8024,  1057,   857,  3229,  1320,  2533,  1168,   749,
          1139,   725,  2692,  3160,  4638,  7711,  1599,  8038,  1184,  5637,
          3302,  1243,  2523,  3984,  7678,  8024,  9537, 10073, 12018,  4638,
          6862,  2428,  2523,  2571,  8024,  7271,  4997,  2523,  4229,  2658,
          8024,   712,  1240,  5183,   872,  2990,  6121,  3330,  8024,  1963,
          3362,  7274,  6722,  1343,  3298,  6250,   857,  4158,   872,  4522,
          1962,  6722,   855,  8039,  2791,  7279,  7481,  4948,   679,  2207,
          8024,  6172,  7617,  2533,  2523,  5125,  5232,  8024,  6917,  3300,
          3362,  4676,  6557,  6843,  8039,  4472,  1862,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([1, 1, 1, 1])}
```

```python
num = 0
for batch in d1:
    print(batch['input_ids'].size())
    num += 1
    if num > 10:
        break
#每個bach有4個,長度超過128只有128,小於128會依據實際的長度

#==output==
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 124])
torch.Size([4, 79])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
```




