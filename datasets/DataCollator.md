## DataCollator

### DataCollator的功能是什麼？

DataCollator是HuggingFace Transformers庫中的一個重要類別，它的主要功能是**在訓練或評估模型時，對資料樣本進行批次處理**。具體來說，DataCollator會將多個單獨的資料樣本組合成一個批次（batch），並對這些樣本進行必要的預處理，以確保批次中的所有樣本具有一致的形狀和結構，適合輸入到模型中進行訓練或推理。

它的常見功能包括：
- **填充（Padding）**：將較短的序列填充到相同長度，通常使用特定的填充標記（padding token）。
- **截斷（Truncation）**：將過長的序列截斷到指定的最大長度。
- **生成注意力掩碼（Attention Masks）**：標識序列中哪些部分是實際內容，哪些是填充部分，以便模型在處理時忽略填充區域。
- **標籤處理**：根據任務需求，對標籤（labels）進行相應的處理，例如對齊輸入序列或轉換格式。

簡單來說，DataCollator是一個資料處理的「幫手」，它幫助開發者將原始資料轉換成模型可以直接使用的格式，從而簡化訓練流程。

---

### 為何會有不同的DataCollator？

之所以有不同的DataCollator，是因為**不同的模型和任務對資料處理的需求不同**。HuggingFace提供了多種專門的DataCollator，以適應各種場景。以下是一些例子來說明原因：

1. **語言模型（Language Models）**  
   對於像BERT或GPT這樣的語言模型，輸入序列需要填充和截斷，並生成注意力掩碼。某些語言模型（如GPT）還需要隨機掩碼（masking）來訓練預測能力，因此需要專門的處理邏輯。

2. **序列到序列模型（Sequence-to-Sequence Models）**  
   在機器翻譯或摘要生成等任務中，除了處理輸入序列，還需要對目標序列（target sequences）進行填充和截斷，並生成對應的標籤。這與單純的語言模型需求不同。

3. **問答模型（Question Answering Models）**  
   問答任務需要處理問題和上下文的配對，並生成答案的起始和結束位置標籤，這要求DataCollator能夠處理這種特殊的結構。

4. **標記分類（Token Classification）**  
   在命名實體識別（NER）等任務中，每個輸入標記（token）都有對應的標籤。DataCollator需要確保標籤與填充後的序列正確對齊。

5. **多標籤分類（Multi-label Classification）**  
   在多標籤任務中，每個樣本可能有多個類別，標籤需要轉換成特定的格式（如二進制向量），這與單標籤分類不同。

---

### 常見的DataCollator類型

HuggingFace根據這些需求提供了多種DataCollator，以下是一些常見的例子：
- **DataCollatorWithPadding**：最基本的類型，負責填充和截斷，適用於大多數語言模型和分類任務。
- **DataCollatorForLanguageModeling**：專為語言模型設計，支援隨機掩碼，適合自迴歸模型（如GPT-2）。
- **DataCollatorForSeq2Seq**：用於序列到序列任務，處理輸入和目標序列，適用於翻譯或摘要生成。
- **DataCollatorForTokenClassification**：用於標記分類任務，確保標籤與序列對齊，適合NER。
- **DataCollatorForQuestionAnswering**：專為問答任務設計，生成起始和結束位置標籤。

---

### 總結

DataCollator的功能是**將資料樣本批次化並進行預處理**，以滿足模型輸入的需求。而不同的DataCollator存在是因為**不同的任務和模型需要特定的資料處理方式**。這種靈活性讓開發者可以根據自己的需求選擇合適的DataCollator，從而提高資料準備的效率並簡化模型訓練過程。

### 實作,使用DataCollatorWithPadding

- 每一批次自動產生對應的padding
- 所以每一批次會依據內容,產生不同的批次

```python
from transformers import DataCollatorWithPadding
from datasets import load_dataset


dataset = load_dataset("csv",data_files="./ChnSentiCorp_htl_all.csv",split="train")
dataset

#==output==
dataset = load_dataset("csv",data_files="./ChnSentiCorp_htl_all.csv",split="train")
dataset
```

**清理資料**

```python
#清理資料
cleared_dataset = dataset.filter(lambda item: item['review'] is not None)
cleared_dataset

#==output==
Dataset({
    features: ['label', 'review'],
    num_rows: 7765
})
```

**分詞每筆資料**

```python
from transformers import AutoTokenizer
import torch

from transformers import AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')
def process_tokenizer(example:dict)->dict:
    '''
    只要先分詞,不要現在轉成tensor,轉成tensor,由DataCollator來作
    '''
    tokenized = tokenizer(example['review'],max_length=128,truncation=True)
    tokenized['label'] = example['label']
    return tokenized


tokenizer_dataset = cleared_dataset.map(function=process_tokenizer,remove_columns=cleared_dataset.column_names)
tokenizer_dataset[:2]
      
```

```python
collator = DataCollatorWithPadding(tokenizer=tokenizer)
collator

#==output==
DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	100: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	101: AddedToken("[CLS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	102: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	103: AddedToken("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')
```


```python
from torch.utils.data import DataLoader
d1 = DataLoader(tokenizer_dataset, batch_size=4, collate_fn=collator, shuffle=True)

next(enumerate(d1))[1] #可以發現已經轉成tensor

#==output==
{'input_ids': tensor([[  101,  2769,   947,  3176,  8126,  3189,   678,  1286,  2537,  6832,
          3172,  2225,  1962,  5844,  1854,  6983,  2421,   889,  1168,  2356,
          1281,  1057,   857,  6857,  2157,  7613,  2421,  8024,  7613,  2421,
          1762,   671,   943,   100,   100,  6174,  7481,  8024,  3760,   889,
          6882,  4638,  6917,  4696,   679,  6210,  2533,  5543,  2523,  3175,
           912,  4638,  2823,  1168,   511,  6794,  4415,  4638,  6862,  2428,
          2523,  2571,  8024,  2706,  2428,   738,  2523,  1962,  8024,  1920,
          5147,   126,  1146,  7132,  2218,  3018,  2137,   511,   889,  1168,
          2791,  7279,  2527,  6366,  2769,   947,  3300,  7953,   679,  6900,
          2746,  8024,  1728,  4158,  4696,  4638,  2523,  2207,  8024,  2215,
          1071,  3221,  1190,   857,  6882,  6832,  3172,  2225,  1962,  5844,
          1854,  6983,  2421,  8024,  2791,  7279,  7481,  4948,  1920,  5147,
          1372,  3300,  8108,  2398,  5101,  2340,  1381,   102],
        [  101,  2791,  7279,  3683,  6733,   746,  3912,  8024,  6127,  4495,
          7279,  1922,  2207,  8024,  6752,  6716,  6963,  6527,  1233,   511,
          1765,  4415,   855,  5390,   671,  1288,  8024,  1728,  1762,   100,
           100,  6174,  8024,   679,  2159,  3211,  2823,  1168,   511,  3302,
          1243,   671,  5663,   511,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  2791,  7279,  7478,  2382,  1920,  8024,  4801,  7768,  6257,
          3177,  6917,   679,  7097,  8024,   679,  6882,  3193,  7623,  2345,
           749,  7953,  8024,  2791,  1019,   738,  2253,  3176,  3683,  6733,
          6523,  4638,  6540,  7631,  1353,  7637,  8182,  2399,   126,  3299,
          8113,  3189,  8038,  2644,  1962,  8013,  2697,  6342,  2644,  4638,
          2456,  2692,  8024,  6983,  2421,  4638,  3193,  7623,  1139,  1501,
          4412,  3633,   976,  6310,  3146,  8024,  3309,  3307,  2644,  4638,
           678,  3613,  1045,  5631,  3298,  6888,  1168,  2644,  4638,  4021,
          2692,  8013,  6296,  3041,  4638,  3631,  6816,  2644,  4638,   678,
          3613,  1045,  5631,  8013,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  7521,  6242,  3862,  3250,  5709,  1754,  1920,  6983,  2421,
          2347,  5195,  3760,  3300,  2791,  7279,   749,  8024,  3176,  3221,
          2994,  1168,   749,  6857,  6174,  8024,  4692,  1168,  1912,  6134,
          3300,   763,  4542,  2663,  8024,  1728,  4158,  6983,  2421,   699,
           679,  1920,  8024,  1057,   857,  3229,  1320,  2533,  1168,   749,
          1139,   725,  2692,  3160,  4638,  7711,  1599,  8038,  1184,  5637,
          3302,  1243,  2523,  3984,  7678,  8024,  9537, 10073, 12018,  4638,
          6862,  2428,  2523,  2571,  8024,  7271,  4997,  2523,  4229,  2658,
          8024,   712,  1240,  5183,   872,  2990,  6121,  3330,  8024,  1963,
          3362,  7274,  6722,  1343,  3298,  6250,   857,  4158,   872,  4522,
          1962,  6722,   855,  8039,  2791,  7279,  7481,  4948,   679,  2207,
          8024,  6172,  7617,  2533,  2523,  5125,  5232,  8024,  6917,  3300,
          3362,  4676,  6557,  6843,  8039,  4472,  1862,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([1, 1, 1, 1])}
```

```python
num = 0
for batch in d1:
    print(batch['input_ids'].size())
    num += 1
    if num > 10:
        break
#每個bach有4個,長度超過128只有128,小於128會依據實際的長度

#==output==
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 124])
torch.Size([4, 79])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
torch.Size([4, 128])
```




