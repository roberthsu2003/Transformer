{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "from pprint import pprint\n",
    "pprint(SUPPORTED_TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#列印出所有支援的任務和任務的類型\n",
    "for k, v in SUPPORTED_TASKS.items():\n",
    "    print(k)\n",
    "    print(\"=========================\")\n",
    "    for k1, v1 in v.items():\n",
    "        print(f\"{k1}:{v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先在上方輸出的技援的任務內,搜尋是否有支援的text-classfication任務\n",
    "- 進入hugging faces的網站,尋找那一些模型有支援text-classfication任務和支援中文的\n",
    "- 先選擇model:\n",
    "    - 再選擇task(任務)->Natural(自然語言)下的text-classification(文字分類)\n",
    "    - 再選擇languages(語系)->Chinese(中文)\n",
    "- 再將過濾的模型排序\n",
    "此處使用**papluca/xlm-roberta-base-language-detection**\n",
    "- 功能是辨識使用者輸入的文字是那一個語系\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用pipeline模型會被自動下載在本地端\n",
    " - mac,linux ->  ~/.cache/huggingface\n",
    " - windows -> C:\\Users\\username.cache\\huggingface\n",
    "\n",
    " 使用完後可以手動刪除\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"papluca/xlm-roberta-base-language-detection\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ja\",\n",
      "    \"1\": \"nl\",\n",
      "    \"2\": \"ar\",\n",
      "    \"3\": \"pl\",\n",
      "    \"4\": \"de\",\n",
      "    \"5\": \"it\",\n",
      "    \"6\": \"pt\",\n",
      "    \"7\": \"tr\",\n",
      "    \"8\": \"es\",\n",
      "    \"9\": \"hi\",\n",
      "    \"10\": \"el\",\n",
      "    \"11\": \"ur\",\n",
      "    \"12\": \"bg\",\n",
      "    \"13\": \"en\",\n",
      "    \"14\": \"fr\",\n",
      "    \"15\": \"zh\",\n",
      "    \"16\": \"ru\",\n",
      "    \"17\": \"th\",\n",
      "    \"18\": \"sw\",\n",
      "    \"19\": \"vi\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ar\": 2,\n",
      "    \"bg\": 12,\n",
      "    \"de\": 4,\n",
      "    \"el\": 10,\n",
      "    \"en\": 13,\n",
      "    \"es\": 8,\n",
      "    \"fr\": 14,\n",
      "    \"hi\": 9,\n",
      "    \"it\": 5,\n",
      "    \"ja\": 0,\n",
      "    \"nl\": 1,\n",
      "    \"pl\": 3,\n",
      "    \"pt\": 6,\n",
      "    \"ru\": 16,\n",
      "    \"sw\": 18,\n",
      "    \"th\": 17,\n",
      "    \"tr\": 7,\n",
      "    \"ur\": 11,\n",
      "    \"vi\": 19,\n",
      "    \"zh\": 15\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"papluca/xlm-roberta-base-language-detection\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ja\",\n",
      "    \"1\": \"nl\",\n",
      "    \"2\": \"ar\",\n",
      "    \"3\": \"pl\",\n",
      "    \"4\": \"de\",\n",
      "    \"5\": \"it\",\n",
      "    \"6\": \"pt\",\n",
      "    \"7\": \"tr\",\n",
      "    \"8\": \"es\",\n",
      "    \"9\": \"hi\",\n",
      "    \"10\": \"el\",\n",
      "    \"11\": \"ur\",\n",
      "    \"12\": \"bg\",\n",
      "    \"13\": \"en\",\n",
      "    \"14\": \"fr\",\n",
      "    \"15\": \"zh\",\n",
      "    \"16\": \"ru\",\n",
      "    \"17\": \"th\",\n",
      "    \"18\": \"sw\",\n",
      "    \"19\": \"vi\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ar\": 2,\n",
      "    \"bg\": 12,\n",
      "    \"de\": 4,\n",
      "    \"el\": 10,\n",
      "    \"en\": 13,\n",
      "    \"es\": 8,\n",
      "    \"fr\": 14,\n",
      "    \"hi\": 9,\n",
      "    \"it\": 5,\n",
      "    \"ja\": 0,\n",
      "    \"nl\": 1,\n",
      "    \"pl\": 3,\n",
      "    \"pt\": 6,\n",
      "    \"ru\": 16,\n",
      "    \"sw\": 18,\n",
      "    \"th\": 17,\n",
      "    \"tr\": 7,\n",
      "    \"ur\": 11,\n",
      "    \"vi\": 19,\n",
      "    \"zh\": 15\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/model.safetensors\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/roberthsu2003/.cache/huggingface/hub/models--papluca--xlm-roberta-base-language-detection/snapshots/9865598389ca9d95637462f743f683b51d75b87b/tokenizer_config.json\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'en', 'score': 0.8889275789260864}],\n",
       " [{'label': 'it', 'score': 0.9120126962661743}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "#pipeline不支援,本地端的模型下載\n",
    "\n",
    "\n",
    "text = [\n",
    "    \"Brevity is the soul of wit.\",\n",
    "    \"Amor, ch'a nullo amato amar perdona.\"\n",
    "]\n",
    "\n",
    "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "pipe = pipeline(\"text-classification\", model=model_ckpt)\n",
    "results = pipe(text, top_k=1, truncation=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "0.8889275789260864\n",
      "==============\n",
      "it\n",
      "0.9120126962661743\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "for items in results:\n",
    "    print(items[0]['label'])\n",
    "    print(items[0]['score'])\n",
    "    print(\"==============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
