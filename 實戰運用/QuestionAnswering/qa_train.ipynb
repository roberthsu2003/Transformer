{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基於截斷策略的機器閱讀理解任務實現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 載入相關套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2數據集載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset('cmrc2018', cache_dir='data')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-macbert-base')\n",
    "\n",
    "def process_func(examples):\n",
    "    tokenized_examples = tokenizer(text=examples['question'],\n",
    "                               text_pair=examples['context'],\n",
    "                               max_length=512,\n",
    "                               return_offsets_mapping=True,\n",
    "                               truncation=\"only_second\",\n",
    "                               padding=True)\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for idx, offset in enumerate(offset_mapping):\n",
    "        answer = examples['answers'][idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None,context_start) - 1\n",
    "\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            #代表答案不在context中\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -= 1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1caf9d5dd5e4506a839d68c5b232a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained('hfl/chinese-macbert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = 'models_for_qa',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立訓練器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenied_datasets['train'],\n",
    "    eval_dataset=tokenied_datasets['validation'],\n",
    "    data_collator=DefaultDataCollator()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
