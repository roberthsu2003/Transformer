{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自建一個cmrc2018的2筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_json = [{\n",
    "    'id':'lesson1',\n",
    "    'context':'小英的生日是1997年10月9日,女性',\n",
    "    'question':'小英的生日是?',\n",
    "    'answers':{'text':['1997年10月9日'],'answer_start':[6]}\n",
    "    },\n",
    "    {\n",
    "    'id':'lesson2',\n",
    "    'context':'川普日前宣布課徵加拿大和墨西哥25%的關稅',\n",
    "    'question':'加拿大和墨西哥被課徵的關稅是',\n",
    "    'answers':{'text':['25%'],'answer_start':[15]}\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轉換為DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_list(source_json)\n",
    "datasets = DatasetDict({\n",
    "    'train':train_dataset\n",
    "})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目標\n",
    "![](./images/pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用AutoTokenizer建立input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "[101, 2207, 5739, 4638, 4495, 3189, 3221, 136, 102, 2207, 5739, 4638, 4495, 3189, 3221, 8387, 2399, 8108, 3299, 130, 3189, 117, 1957, 2595, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    text=datasets['train']['question'],\n",
    "    text_pair=datasets['train']['context'],\n",
    "    max_length = 512, #BERT最高處理512byte,\n",
    "    truncation=\"only_second\", #全部超過只截斷context,\n",
    "    padding=True\n",
    ")\n",
    "#查資訊類型\n",
    "print(type(tokenized_dataset))\n",
    "#取出第一筆\n",
    "print(tokenized_dataset['input_ids'][0])\n",
    "print(tokenized_dataset['token_type_ids'][0])\n",
    "print(tokenized_dataset['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/pic2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找出tokenize答案的起始index和答案的結束index\n",
    "### 解決方案\n",
    "### 請參考圖片說明\n",
    "\n",
    "- 找出start_char\n",
    "- 找出end_char\n",
    "- 取出offset_mapping\n",
    "- 取出tokenizer的sequence_ids\n",
    "- 找出context_start\n",
    "- 找出context_end\n",
    "- 由前往後找,找出答案的起始index\n",
    "- 由後行前找,找出答案的結束index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/pic3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    text=datasets['train']['question'],\n",
    "    text_pair=datasets['train']['context'],\n",
    "    max_length = 512, \n",
    "    truncation=\"only_second\", \n",
    "    padding=True,\n",
    "    return_offsets_mapping=True #才可以取出offset_mapping\n",
    ")\n",
    "\n",
    "print(tokenized_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出offset_mapping\n",
    "offset_mapping = tokenized_dataset.pop('offset_mapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 10), (10, 11), (11, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "#取出offset_mapping第1筆資料\n",
    "offset = offset_mapping[0]\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找出start_char和end_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_start': [6], 'text': ['1997年10月9日']}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = datasets['train']['answers'][0]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_char = answer['answer_start'][0]\n",
    "start_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_char = start_char + len(answer['text'][0])\n",
    "end_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要取的sequence_ids才可以找出下面的資料\n",
    "## 找出context_start和context_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "context = tokenized_dataset.sequence_ids(0) #取出第0筆的sequence_ids\n",
    "print(context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "context_start = context.index(1) #取出索引1的起始編號\n",
    "print(context_start) #1的起始索引編號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "context_end = context.index(None,context_start) - 1 #取出1後是None的起始編號的前一個(就是1的結束的索引編號)\n",
    "print(context_end) #取出1的最後一個的索引編號\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 由前往後找到start_position\n",
    "## 由後行前找到end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由於截斷可能會沒有答案\n",
    "if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "    #代表答案不在context中\n",
    "    start_token_pos = 0\n",
    "    end_token_pos = 0\n",
    "else:\n",
    "    token_id = context_start\n",
    "    #由前後找,找出起始位置\n",
    "    while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "        token_id += 1\n",
    "    start_token_pos = token_id\n",
    "    token_id = context_end\n",
    "    #由後往前找,找出最後的位置\n",
    "    while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "        token_id -= 1\n",
    "    end_token_pos = token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證是否正確"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8387, 2399, 8108, 3299, 130, 3189]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_token = tokenized_dataset['input_ids'][0][start_token_pos:end_token_pos + 1]\n",
    "answer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1997 年 10 月 9 日'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#驗證正確\n",
    "tokenizer.decode(answer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立function,一次處理一個batch\n",
    "### 並得到訓練時所需要的資料\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "def process_func(examples):\n",
    "    tokenized_examples = tokenizer(text=examples['question'],\n",
    "                               text_pair=examples['context'],\n",
    "                               max_length=512,\n",
    "                               return_offsets_mapping=True,\n",
    "                               truncation=\"only_second\",\n",
    "                               padding=True)\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for idx, offset in enumerate(offset_mapping):\n",
    "        answer = examples['answers'][idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None,context_start) - 1\n",
    "\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            #代表答案不在context中\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -= 1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bbddb45a8345228d6282196ec5a204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets['train'].column_names)\n",
    "tokenied_datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
