{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自建一個cmrc2018的2筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_json = [{\n",
    "    'id':'lesson1',\n",
    "    'context':'小英的生日是1997年10月9日,女性',\n",
    "    'question':'小英的生日是?',\n",
    "    'answer':{'text':['1997年10月9日'],'answer_start':[6]}\n",
    "    },\n",
    "    {\n",
    "    'id':'lesson2',\n",
    "    'context':'川普日前宣布課徵加拿大和墨西哥25%的關稅',\n",
    "    'question':'加拿大和墨西哥被課徵的關稅是',\n",
    "    'answer':{'text':['25%'],'answer_start':[15]}\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轉換為DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answer'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_list(source_json)\n",
    "datasets = DatasetDict({\n",
    "    'train':train_dataset\n",
    "})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目標\n",
    "![](./images/pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用AutoTokenizer建立input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "[101, 2207, 5739, 4638, 4495, 3189, 3221, 136, 102, 2207, 5739, 4638, 4495, 3189, 3221, 8387, 2399, 8108, 3299, 130, 3189, 117, 1957, 2595, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    text=datasets['train']['question'],\n",
    "    text_pair=datasets['train']['context'],\n",
    "    max_length = 512, #BERT最高處理512byte,\n",
    "    truncation=\"only_second\", #全部超過只截斷context,\n",
    "    padding=True\n",
    ")\n",
    "#查資訊類型\n",
    "print(type(tokenized_dataset))\n",
    "#取出第一筆\n",
    "print(tokenized_dataset['input_ids'][0])\n",
    "print(tokenized_dataset['token_type_ids'][0])\n",
    "print(tokenized_dataset['attention_mask'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
