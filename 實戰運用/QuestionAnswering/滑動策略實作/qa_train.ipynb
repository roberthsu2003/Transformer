{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberthsu2003/Transformer/blob/main/%E5%AF%A6%E6%88%B0%E9%81%8B%E7%94%A8/QuestionAnswering/%E6%BB%91%E5%8B%95%E7%AD%96%E7%95%A5%E5%AF%A6%E4%BD%9C/qa_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNHiImn-zrWU",
        "outputId": "a248365f-0a0b-48a0-aea5-76f039c29008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "#cmrc_eval.py檔會用到這個套件\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PVuzhmPXzrWV",
        "outputId": "e45a955c-416e-4f11-ac5f-ea2151b822e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cmrc_eval.py'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wget\n",
        "#cmrc_eval.py評估檔內容有修改過\n",
        "wget.download('https://raw.githubusercontent.com/roberthsu2003/Transformer/refs/heads/main/for_download/cmrc_eval.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 基於視窗滑動策略的機器閱讀理解(MRC)\n",
        "### 載人套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,TrainingArguments,Trainer, DefaultDataCollator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 下載資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"roberthsu2003/for_MRC_QA\", cache_dir='data')\n",
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 數據處理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#建立處理token的function\n",
        "def process_func(examples):\n",
        "    tokenized_example = tokenizer(\n",
        "        text = examples[\"question\"],\n",
        "        text_pair=examples['context'],\n",
        "        return_offsets_mapping=True,\n",
        "        return_overflowing_tokens=True,\n",
        "        stride = 128, #設定重疊的部份\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        padding=\"max_length\"\n",
        "        )\n",
        "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "    \n",
        "    for idx, _ in enumerate(sample_mapping):    \n",
        "        answer = examples['answers'][sample_mapping[idx]] #參考白板比較好理解\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "\n",
        "        context_start = tokenized_example.sequence_ids(idx).index(1)\n",
        "        context_end = tokenized_example.sequence_ids(idx).index(None,context_start)-1\n",
        "\n",
        "        offset = tokenized_example.get(\"offset_mapping\")[idx]\n",
        "\n",
        "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
        "            #答案不在context內\n",
        "            start_token_pos = 0\n",
        "            end_token_pos = 0\n",
        "        else:\n",
        "            #由左而右再由右而左找尋答案的index\n",
        "            token_id = context_start\n",
        "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
        "                token_id += 1\n",
        "            start_token_pos = token_id\n",
        "            token_id = context_end\n",
        "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
        "                token_id -= 1\n",
        "            end_token_pos = token_id\n",
        "\n",
        "        start_positions.append(start_token_pos)\n",
        "        end_positions.append(end_token_pos)\n",
        "        example_ids.append(examples[\"id\"][sample_mapping[idx]])\n",
        "        #這些程式碼是為了預測使用的\n",
        "        tokenized_example[\"offset_mapping\"][idx] = [\n",
        "            (o if tokenized_example.sequence_ids(idx)[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_example[\"offset_mapping\"][idx])\n",
        "        ]\n",
        "\n",
        "\n",
        "    tokenized_example[\"example_ids\"] = example_ids\n",
        "    tokenized_example[\"start_positions\"] = start_positions\n",
        "    tokenized_example[\"end_positions\"] = end_positions\n",
        "    return tokenized_example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 獲取模型輸出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "def get_result(start_logits, end_logits, exmaples, features):\n",
        "\n",
        "    predictions = {}\n",
        "    references = {}\n",
        "\n",
        "    # example 和 feature的映射\n",
        "    example_to_feature = collections.defaultdict(list)\n",
        "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
        "        example_to_feature[example_id].append(idx)\n",
        "\n",
        "    # 最优答案候选\n",
        "    n_best = 20\n",
        "    # 最大答案长度\n",
        "    max_answer_length = 30\n",
        "\n",
        "    for example in exmaples:\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "        for feature_idx in example_to_feature[example_id]:\n",
        "            start_logit = start_logits[feature_idx]\n",
        "            end_logit = end_logits[feature_idx]\n",
        "            offset = features[feature_idx][\"offset_mapping\"]\n",
        "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if offset[start_index] is None or offset[end_index] is None:\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    answers.append({\n",
        "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
        "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
        "                    })\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
        "            predictions[example_id] = best_answer[\"text\"]\n",
        "        else:\n",
        "            predictions[example_id] = \"\"\n",
        "        references[example_id] = example[\"answers\"][\"text\"]\n",
        "\n",
        "    return predictions, references"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 評估函數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cmrc_eval import evaluate_cmrc\n",
        "\n",
        "def metirc(pred):\n",
        "    start_logits, end_logits = pred[0]\n",
        "    if start_logits.shape[0] == len(tokenized_datasets[\"validation\"]):\n",
        "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenized_datasets[\"validation\"])\n",
        "    else:\n",
        "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenized_datasets[\"test\"])\n",
        "    return evaluate_cmrc(p, r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 下載模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(\"google-bert/bert-base-chinese\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 配置TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"models_for_qa_slide\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step8 配置Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=DefaultDataCollator(),\n",
        "    compute_metrics=metirc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 模型訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 模型预测"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
        "pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
