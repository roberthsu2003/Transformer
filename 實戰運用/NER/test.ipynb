{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification,TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset \n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4637\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trust_remote_code,代表信任遠端資料,如不寫會有信任與否的提示\n",
    "ner_datasets = load_dataset(\"peoples_daily_ner\", cache_dir='./data', trust_remote_code=True)\n",
    "ner_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets['train'][0]\n",
    "\n",
    "# id,tokens,ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_datasets['train'].features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取得label_list\n",
    "label_list = ner_datasets['train'].features['ner_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1],\n",
      "                    [1, 1, 1]],\n",
      " 'input_ids': [[101, 3862, 102],\n",
      "               [101, 7157, 102],\n",
      "               [101, 3683, 102],\n",
      "               [101, 6612, 102],\n",
      "               [101, 1765, 102],\n",
      "               [101, 4157, 102],\n",
      "               [101, 1762, 102],\n",
      "               [101, 1336, 102],\n",
      "               [101, 7305, 102],\n",
      "               [101, 680, 102],\n",
      "               [101, 7032, 102],\n",
      "               [101, 7305, 102],\n",
      "               [101, 722, 102],\n",
      "               [101, 7313, 102],\n",
      "               [101, 4638, 102],\n",
      "               [101, 3862, 102],\n",
      "               [101, 1818, 102],\n",
      "               [101, 511, 102]],\n",
      " 'token_type_ids': [[0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0],\n",
      "                    [0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "#不正確,這是以字為單位,而不是以句子為單位\n",
    "from pprint import pprint\n",
    "pprint(tokenizer(ner_datasets['train'][0]['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               3862,\n",
      "               7157,\n",
      "               3683,\n",
      "               6612,\n",
      "               1765,\n",
      "               4157,\n",
      "               1762,\n",
      "               1336,\n",
      "               7305,\n",
      "               680,\n",
      "               7032,\n",
      "               7305,\n",
      "               722,\n",
      "               7313,\n",
      "               4638,\n",
      "               3862,\n",
      "               1818,\n",
      "               511,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer(ner_datasets['train'][0]['tokens'],is_split_into_words=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101, 2207, 3209, 2695, 1343, 3862, 6920, 7037, 7797, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "#文字字數8個加上101,102共10個\n",
    "#input_ids = 101+字數+102,共10個\n",
    "#所以labels也必需要10個\n",
    "pprint(tokenizer(['小','明','愛','去','海','邊','釣','魚'],is_split_into_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               2207,\n",
      "               3209,\n",
      "               10673,\n",
      "               12865,\n",
      "               8415,\n",
      "               8228,\n",
      "               3862,\n",
      "               6920,\n",
      "               7037,\n",
      "               7797,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "#中英文混合\n",
    "#文字字數8個加上101,102共10個\n",
    "#input_ids = 101+10個字數+102,共12個 -> 英文字有sub_word的關係\n",
    "#所以labels必需要12個\n",
    "pprint(tokenizer(['小','明','interest','to','海','邊','釣','魚'],is_split_into_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               2207,\n",
      "               3209,\n",
      "               10673,\n",
      "               12865,\n",
      "               8415,\n",
      "               8228,\n",
      "               3862,\n",
      "               6920,\n",
      "               7037,\n",
      "               7797,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, None]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer(['小','明','interest','to','海','邊','釣','魚'],is_split_into_words=True))\n",
    "ner_tags = [1, 2, 0, 0, 0, 0, 0, 0]\n",
    "#input_ids = [101,2207,3209,10673,12865,8415,8228,3862,6920,7037,7797,102]\n",
    "#101,102,要產生-100\n",
    "#我們預計要產生的labels必需是=[-100,1,2,0,0,0,0,0,0,0,0,-100],共12,滿足input_ids的數量,但ner_tags欄位不可以滿足\n",
    "#使用word_ids()解決問題\n",
    "res = tokenizer(['小','明','interest','to','海','邊','釣','魚'],is_split_into_words=True)\n",
    "print(res.word_ids())\n",
    "#[None, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, None]\n",
    "#None -> 101, 102\n",
    "#0 -> '小'\n",
    "#1 -> '明'\n",
    "#2,2,2 -> 'interest'\n",
    "#3 -> 'to'\n",
    "#4 -> '海'\n",
    "#5 -> '邊'\n",
    "#6 -> '釣'\n",
    "#7 -> '魚'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "ner_tags = [1, 2, 0, 0, 0, 0, 0, 0] #自已手動的\n",
    "res = tokenizer(['小','明','interest','to','海','邊','釣','魚'],max_length=128,truncation=True,is_split_into_words=True)\n",
    "word_id = res.word_ids()\n",
    "label_id = []\n",
    "for id in word_id:\n",
    "    if id is None:\n",
    "        label_id.append(-100)\n",
    "    else:\n",
    "        #print(id)\n",
    "        label_id.append(ner_tags[id])\n",
    "print(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20865\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2319\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4637\n",
      "    })\n",
      "})\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "#1筆1筆處理的方法\n",
    "def process_one(example):\n",
    "    ner_tags = example['ner_tags']\n",
    "    res = tokenizer(example['tokens'],max_length=128, truncation=True, is_split_into_words=True)\n",
    "    word_id = res.word_ids()\n",
    "    label_id = []\n",
    "    for id in word_id:\n",
    "        if id is None:\n",
    "            label_id.append(-100)\n",
    "        else:\n",
    "            label_id.append(ner_tags[id])\n",
    "    res['labels'] = label_id\n",
    "\n",
    "    return res\n",
    "tokenized_datasets = ner_datasets.map(process_one)\n",
    "print(tokenized_datasets)\n",
    "pprint(tokenized_datasets['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20865\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2319\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4637\n",
      "    })\n",
      "})\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "#一個批次處理的方式\n",
    "from pprint import pprint\n",
    "def process_batch(examples):\n",
    "    tokenized_examples = tokenizer(examples['tokens'], max_length=128, truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_examples.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_examples['labels'] = labels\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = ner_datasets.map(process_batch,batched=True)\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#label_list\n",
    "#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-chinese\",num_labels=len(label_list)) #預設是2個,現在是7個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查詢模型的num_labels\n",
    "model.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqeval = evaluate.load('seqeval')\n",
    "seqeval #查看所需要的說明書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    #print(predictions),評估時print()可以比較了解\n",
    "    predictions = np.argmax(predictions, axis=-1) #變為和label一樣的一維資料\n",
    "\n",
    "    #刪除-100\n",
    "    truth_predictions = [\n",
    "        [label_list[p] for p,l in zip(prediction, label) if p != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    truth_labels = [\n",
    "        [label_list[l] for p,l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    result = seqeval.compute(predictions=truth_predictions, references=truth_labels, mode='strict', scheme='IOB2')\n",
    "    return{\n",
    "        \"f1\": result['overall_f1']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = \"models_for_ner\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=128,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='f1',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps = 50,\n",
    "    num_train_epochs=3,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
