## Model簡介
### Transformer的重要架構
1. The encoder-decoder framework
2. Attention mechanisms
3. Transfer learning

#### The encoder-decoder framework
- 比Transformer興起時就有的架構
在transformer之前，LSTM 等循環架構是 NLP 的最先進技術。這些架構在網路連線中包含一個回饋迴路，可讓資訊從一個步驟傳播到另一個步驟，因此非常適合用來建模文字等序列資料。

![](./images/pic1.png)

一個 RNN 接收一些輸入 (可能是單字或字元)，將其饋入網路，並輸出一個稱為隱藏狀態的向量(**hidden state**)。 與此同時，模型會透過回饋迴路將一些資訊回饋給它自己，然後在下一步中使用這些資訊。如圖所示：RNN 將每一步的狀態資訊傳遞到序列中的下一個操作。這些架構曾經（並將繼續）廣泛應用於 NLP 任務、語音處理和時間序列。RNN 發揮重要作用的一個領域是機器翻譯系統的開發，其目標是將一種語言的字詞序列映射到另一種語言。此類任務通常透過編碼器-解碼器或序列到序列架構，非常適合輸入和輸出都是任意長度序列的情況。編碼器的工作是將輸入序列的資訊編碼成一個數字表示，通常稱為最後的隱藏狀態。

![](./images/pic2.png)

如圖所示:一對 RNN 對此進行了說明，其中英語句子“Transformers are Great!”被編碼為隱藏狀態向量，然後被解碼以產生德語翻譯“Transformer sindgrosartig！”輸入字按順序饋送通過編碼器，並且從上到下一次產生一個輸出字。

儘管這種架構簡單優雅，但其弱點是編碼器的最終隱藏狀態造成了資訊瓶頸(_**information bottleneck**_)：它必須代表整個輸入序列的意義，因為這是解碼器在產生輸出時所能存取的全部資訊。這對於長序列來說尤其具有挑戰性，因為在將所有內容壓縮為單一固定表示法的過程中，序列開頭的資訊可能會遺失。

幸運的是，有一種方法可以擺脫這個瓶頸，即允許解碼器存取所有編碼器隱藏狀態。這一般的機制稱為注意力(_**attention**_)，是許多現代神經網路架構的關鍵元件。了解 RNN 的注意力是如何開發出來的，將有助於我們了解 Transformer 架構的主要構成元素之一。

#### Attention機制



