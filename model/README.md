## Model簡介
### Transformer的重要架構
1. The encoder-decoder framework
2. Attention mechanisms
3. Transfer learning

#### The encoder-decoder framework
- 比Transformer興起時就有的架構
在transformer之前，LSTM 等循環架構是 NLP 的最先進技術。這些架構在網路連線中包含一個回饋迴路，可讓資訊從一個步驟傳播到另一個步驟，因此非常適合用來建模文字等序列資料。

![](./images/pic1.png)

一個 RNN 接收一些輸入 (可能是單字或字元)，將其饋入網路，並輸出一個稱為隱藏狀態的向量(**hidden state**)。 與此同時，模型會透過回饋迴路將一些資訊回饋給它自己，然後在下一步中使用這些資訊。如圖所示：RNN 將每一步的狀態資訊傳遞到序列中的下一個操作。這些架構曾經（並將繼續）廣泛應用於 NLP 任務、語音處理和時間序列。RNN 發揮重要作用的一個領域是機器翻譯系統的開發，其目標是將一種語言的字詞序列映射到另一種語言。此類任務通常透過編碼器-解碼器或序列到序列架構，非常適合輸入和輸出都是任意長度序列的情況。編碼器的工作是將輸入序列的資訊編碼成一個數字表示，通常稱為最後的隱藏狀態。然後將該狀態傳遞給解碼器，解碼器產生輸出序列。一般而言，編碼器和解碼器元件可以是任何一種可以對序列建模的神經網路架構。

![](./images/pic2.png)




