{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入數據集資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fd7beed3ee47b0ba62f937557269c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "'''\n",
    "dataset_dict = load_dataset(\"csv\",data_files=\"./ChnSentiCorp_htl_all.csv\") #split不指定會傳出DatasetDict\n",
    "dataset_dict['train']\n",
    "'''\n",
    "#整合上面2行成為1行的語法\n",
    "dataset = load_dataset(\"csv\",data_files='./ChnSentiCorp_htl_all.csv', split=\"train\")\n",
    "dataset\n",
    "#清理資料\n",
    "dataset = dataset.filter(lambda example: example['review'] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據集分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "數據集分詞處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "def tokenizer_process(example:dict[str,any]) -> dict[str,any]:\n",
    "    tokenized_example:dict = tokenizer(example['review'], max_length=128, truncation=True)\n",
    "    tokenized_example['labels'] = example['label']\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenizer_process,batched=True,remove_columns=datasets['train'].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得預訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立評估函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "#acc_metric = evaluate.load('accuracy')\n",
    "#f1_metric = evaluate.load('f1')\n",
    "acc_metric = evaluate.load('evaluate-main/metrics/accuracy/accuracy.py')\n",
    "f1_metric = evaluate.load('evaluate-main/metrics/f1/f1.py')\n",
    "\n",
    "def eval_metric(eval_predict):\n",
    "    predictions, labels = eval_predict\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    acc.update(f1)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model='f1',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainingArguments 是 Hugging Face transformers 中 Trainer 類別的設定參數，主要用於控制模型訓練的各種行為。讓我們詳細解析你的 TrainingArguments 設定：\n",
    "\n",
    "1. 輸出目錄 (output_dir)\n",
    "\n",
    "output_dir='./checkpoints'\n",
    "\n",
    "\t•\t作用：指定訓練過程中保存模型檔案的位置。\n",
    "\t•\t原因：這樣可以在訓練過程中自動保存檢查點（checkpoints），以便在訓練中斷時能夠繼續訓練，或用來做推論（inference）。\n",
    "\t•\t注意：最好確保這個目錄存在，並且有足夠的磁碟空間來存放權重檔案。\n",
    "\n",
    "2. 每個裝置的訓練批次大小 (per_device_train_batch_size)\n",
    "\n",
    "per_device_train_batch_size=64\n",
    "\n",
    "\t•\t作用：設定單個 GPU（或 CPU）上的訓練批次大小。\n",
    "\t•\t原因：較大的批次大小有助於更穩定的梯度更新，但會消耗更多的記憶體（VRAM）。64 是一個較大的值，適合高效能 GPU。\n",
    "\t•\t調整建議：\n",
    "\t•\t若 GPU 記憶體不足，應降低此值，如 32 或 16。\n",
    "\t•\t若使用多個 GPU，總批次大小 = per_device_train_batch_size × GPU 數量。\n",
    "\n",
    "3. 每個裝置的評估批次大小 (per_device_eval_batch_size)\n",
    "\n",
    "per_device_eval_batch_size=128\n",
    "\n",
    "\t•\t作用：設定單個 GPU（或 CPU）上的評估批次大小。\n",
    "\t•\t原因：在評估時不需要計算梯度，因此可以使用更大的批次大小來加快評估速度。\n",
    "\t•\t調整建議：\n",
    "\t•\t如果顯示記憶體充足，可以適當提高這個值來加速評估。\n",
    "\n",
    "4. 記錄 (logging_steps)\n",
    "\n",
    "logging_steps=10\n",
    "\n",
    "\t•\t作用：設定每 10 個步驟記錄一次訓練指標（如 loss）。\n",
    "\t•\t原因：讓使用者可以監控訓練進度，而不會因為太頻繁的記錄而影響效能。\n",
    "\t•\t調整建議：\n",
    "\t•\t若想要更細緻的監控，可減少這個值（如 5）。\n",
    "\t•\t若訓練步驟過多，可能需要增加此值來減少 log 頻率。\n",
    "\n",
    "5. 評估策略 (evaluation_strategy)\n",
    "\n",
    "evaluation_strategy=\"epoch\"\n",
    "\n",
    "\t•\t作用：設定評估（validation）的頻率。\n",
    "\t•\t可選值：\n",
    "\t•\t\"no\"：不做評估。\n",
    "\t•\t\"steps\"：每隔 eval_steps 設定的步驟進行一次評估。\n",
    "\t•\t\"epoch\"：每個 epoch 結束時進行評估。\n",
    "\t•\t原因：這裡選擇 \"epoch\"，表示在每個完整的訓練週期結束後，執行一次評估，以確保模型的訓練效果。\n",
    "\n",
    "6. 模型儲存策略 (save_strategy)\n",
    "\n",
    "save_strategy=\"epoch\"\n",
    "\n",
    "\t•\t作用：設定模型的儲存頻率。\n",
    "\t•\t可選值：\n",
    "\t•\t\"no\"：不儲存模型。\n",
    "\t•\t\"steps\"：每 save_steps 設定的步驟存一次。\n",
    "\t•\t\"epoch\"：每個完整的 epoch 後存一次。\n",
    "\t•\t原因：與 evaluation_strategy 一致，每個 epoch 後存一次最佳檢查點，方便後續微調或恢復訓練。\n",
    "\n",
    "7. 最大保存的檢查點數量 (save_total_limit)\n",
    "\n",
    "save_total_limit=3\n",
    "\n",
    "\t•\t作用：最多保留 3 個檢查點，超過這個數量後會自動刪除舊的檢查點。\n",
    "\t•\t原因：\n",
    "\t•\t若不限制，可能會占用大量磁碟空間。\n",
    "\t•\t3 代表保留最近 3 次的最佳模型，足夠進行回溯與選擇。\n",
    "\n",
    "8. 學習率 (learning_rate)\n",
    "\n",
    "learning_rate=2e-5\n",
    "\n",
    "\t•\t作用：設定 AdamW 優化器的學習率（Learning Rate, LR）。\n",
    "\t•\t原因：\n",
    "\t•\t2e-5（0.00002）是適合 Transformer 模型的預設微調學習率。\n",
    "\t•\t若學習率過高，模型可能難以收斂（loss 波動大）。\n",
    "\t•\t若學習率過低，訓練速度變慢。\n",
    "\t•\t調整建議：\n",
    "\t•\t若模型訓練不穩定，可嘗試降低，如 1e-5 或 5e-6。\n",
    "\t•\t若模型訓練過慢且 loss 平穩，可提高學習率，如 3e-5。\n",
    "\n",
    "9. 權重衰減 (weight_decay)\n",
    "\n",
    "weight_decay=0.01\n",
    "\n",
    "\t•\t作用：L2 正則化，用於防止過擬合，讓權重更新時適當衰減。\n",
    "\t•\t原因：\n",
    "\t•\t0.01 是較常見的 Transformer 權重衰減值。\n",
    "\t•\t若模型容易過擬合，可以適當提高，如 0.02。\n",
    "\t•\t若模型學習緩慢或效果不好，可以降低此值，如 0.001。\n",
    "\n",
    "10. 最佳模型的評估指標 (metric_for_best_model)\n",
    "\n",
    "metric_for_best_model='f1'\n",
    "\n",
    "\t•\t作用：設定用來選擇最佳模型的評估指標。\n",
    "\t•\t原因：\n",
    "\t•\tf1 適用於不平衡數據集，因為它是 Precision 和 Recall 的加權平均。\n",
    "\t•\t若是回歸問題，可改為 mse 或 mae。\n",
    "\t•\t若是分類問題，也可使用 accuracy。\n",
    "\n",
    "11. 載入最佳模型 (load_best_model_at_end)\n",
    "\n",
    "load_best_model_at_end=True\n",
    "\n",
    "\t•\t作用：訓練結束時，自動加載評估指標最高（最佳）的模型。\n",
    "\t•\t原因：\n",
    "\t•\t可確保最後得到的是最佳檢查點，而不是最後一個檢查點（因為最後一個可能不是最好的）。\n",
    "\t•\t如果 metric_for_best_model 是 f1，那麼這個選項會加載 f1 最高的模型。\n",
    "\n",
    "總結\n",
    "\n",
    "參數\t作用\t設定值\t說明\n",
    "output_dir\t儲存檢查點的目錄\t'./checkpoints'\t避免訓練中斷後丟失模型\\\n",
    "\n",
    "per_device_train_batch_size\t訓練批次大小\t64\t取決於 GPU 記憶體大小\n",
    "\n",
    "per_device_eval_batch_size\t評估批次大小\t128\t評估時可設較大值\n",
    "\n",
    "logging_steps\t訓練日誌頻率\t10\t控制 log 的頻率\n",
    "\n",
    "evaluation_strategy\t訓練期間何時評估\t\"epoch\"\t每個 epoch 結束後評估\n",
    "\n",
    "save_strategy\t訓練期間何時存模型\t\"epoch\"\t每個 epoch 後存\n",
    "\n",
    "save_total_limit\t保留多少個檢查點\t3\t避免磁碟空間不足\n",
    "\n",
    "learning_rate\t學習率\t2e-5\tTransformer 微調的常見值\n",
    "\n",
    "weight_decay\t權重衰減\t0.01\t防止過擬合\n",
    "\n",
    "metric_for_best_model\t最佳模型的指標\t'f1'\t適用於分類任務\n",
    "\n",
    "load_best_model_at_end\t訓練結束後是否載入最佳模型\tTrue\t取最好的模型\n",
    "\n",
    "這些參數設定適合 Transformer 微調，但可以根據硬體資源與數據集特性進行調整。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
