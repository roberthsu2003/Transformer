{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a19ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50fc3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"roberthsu2003/for_llama3_Instruct\",split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe4978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '給出三個保持健康的提示。', 'role': 'system'}, {'content': '', 'role': 'user'},\n",
      " {'content': '1. 飲食均衡，確保攝取足夠的水果和蔬菜。\\n2. 定期運動，保持身體活躍和強壯。\\n3. 睡眠充足，保持一致的睡眠時間表。',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[0]['conversations'],compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a81725",
   "metadata": {},
   "source": [
    "### 建立tokenizer和model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7462b9",
   "metadata": {},
   "source": [
    "# RoPE (Rotary Position Embedding) 擴充說明\n",
    "\n",
    "RoPE (Rotary Position Embedding) 是一種位置編碼方法，主要用於 Transformer 模型中，讓模型能夠理解序列中token的相對位置關係。以下是詳細說明：\n",
    "\n",
    "### 基本概念\n",
    "- RoPE 通過將 token 的向量表示進行旋轉變換來編碼位置信息\n",
    "- 相比傳統的位置編碼，RoPE 具有更好的外推性能\n",
    "\n",
    "### 主要特點\n",
    "1. **相對位置感知**\n",
    "   - RoPE 能夠讓模型更好地理解 tokens 之間的相對距離關係\n",
    "   - 不受序列絕對位置的限制\n",
    "\n",
    "2. **外推能力**\n",
    "   - 模型在訓練時使用較短序列長度（如2048）\n",
    "   - 推理時可以處理更長的序列（如4096或更長）\n",
    "\n",
    "3. **計算效率**\n",
    "   - 實現簡單且計算開銷較小\n",
    "   - 不需要額外的記憶體來存儲位置編碼\n",
    "\n",
    "### 擴充原理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 #任意選擇！我們內部自動支援 RoPE 擴充！ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcedbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設原本訓練序列長度為 2048\n",
    "max_seq_length = 2048\n",
    "\n",
    "# 可以透過 RoPE 擴充到更長序列\n",
    "extended_seq_length = 4096  # 擴充後的長度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6355d28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 使用場景\n",
    "- 長文本生成\n",
    "- 文檔摘要\n",
    "- 代碼補全\n",
    "- 其他需要處理長序列的任務\n",
    "\n",
    "RoPE 擴充使得模型能夠處理比訓練時更長的序列，這對於需要處理長文本的應用場景特別有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497d7278",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m \u001b[38;5;66;03m#任意選擇！我們內部自動支援 RoPE 擴充！\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/unsloth/__init__.py:87\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Torch 2.5 has including_emulation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m major_version, minor_version = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m SUPPORTS_BFLOAT16 = (major_version >= \u001b[32m8\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (major_torch == \u001b[32m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (minor_torch >= \u001b[32m5\u001b[39m): \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/cuda/__init__.py:507\u001b[39m, in \u001b[36mget_device_capability\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_capability\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    495\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[32m    496\u001b[39m \n\u001b[32m    497\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m \u001b[33;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     prop = \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m prop.major, prop.minor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/cuda/__init__.py:523\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    513\u001b[39m \n\u001b[32m    514\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    524\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/cuda/__init__.py:310\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 #任意選擇！我們內部自動支援 RoPE 擴充！\n",
    "dtype = None #自動偵測,Float16 適用於 Tesla T4、V100，Bfloat16 適用於 Ampere+\n",
    "load_in_4bit = True #使用4bit量化來減少記憶體使用。可以是false\n",
    "\n",
    "# 我們支援 4 位元預量化模型，實現 4 倍更快的下載速度 + 無 OOM。\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
