{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load('accuracy')\n",
    "accuracy.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "#由於無法直接載入 accuracy.py，所以使用 evaluate.load() 載入 accuracy 模組\n",
    "#先至github下載 https://github.com/huggingface/evaluate\n",
    "#accuracy = evaluate.load(\"accuracy\")\n",
    "accuracy = evaluate.load(\"evaluate-main/metrics/accuracy/accuracy.py\")\n",
    "print(accuracy.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"evaluate-main/metrics/accuracy/accuracy.py\")\n",
    "results = accuracy.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"evaluate-main/metrics/accuracy/accuracy.py\")\n",
    "for ref, pred in zip([0, 1, 0, 1], [1, 0, 0, 1]):\n",
    "    accuracy.add(reference=ref, prediction=pred)\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"evaluate-main/metrics/accuracy/accuracy.py\")\n",
    "for refs, preds in zip([[0,1],[0,1]],[[1,0],[0,1]]):\n",
    "    accuracy.add_batch(references=refs, predictions=preds)\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'recall': 0.5,\n",
       " 'precision': 1.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "#clf_metrics = evaluate.combine(['accuracy','f1', 'recall', \"precision\"])\n",
    "clf_metrics = evaluate.combine([\n",
    "    'evaluate-main/metrics/accuracy/accuracy.py',\n",
    "    'evaluate-main/metrics/f1/f1.py',\n",
    "    'evaluate-main/metrics/recall/recall.py',\n",
    "    \"evaluate-main/metrics/precision/precision.py\"])\n",
    "\n",
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.visualization import radar_plot\n",
    "\n",
    "data = [\n",
    "    {\"accuracy\": 0.8, \"f1\": 0.7, \"recall\": 0.9, \"precision\": 0.6},\n",
    "    {\"accuracy\": 0.6, \"f1\": 0.8, \"recall\": 0.7, \"precision\": 0.5},\n",
    "    {\"accuracy\": 0.7, \"f1\": 0.6, \"recall\": 0.8, \"precision\": 0.4},\n",
    "    {\"accuracy\": 0.5, \"f1\": 0.7, \"recall\": 0.6, \"precision\": 0.3},\n",
    "]\n",
    "model_names = [\"model1\", \"model2\", \"model3\", \"model4\"]\n",
    "\n",
    "plot = radar_plot(data, model_names=model_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
